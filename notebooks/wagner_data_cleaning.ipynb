{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd4551a-beb4-44c2-b13c-7fccf8424246",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185cb4e8-7200-4e53-a013-b6a1ec12070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on\n",
    "#https://www.kaggle.com/code/ragnisah/text-data-cleaning-tweets-analysis/notebook\n",
    "#https://github.com/stacktecnologias/stack-repo/blob/master/tweet_tokenize_artigo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de7a32cb-a3be-47d3-bc13-b41fd38be909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: 154kB [00:00, 39.4MB/s]\n",
      "2022-06-03 14:48:07 INFO: Downloading default packages for language: pt (Portuguese)...\n",
      "2022-06-03 14:48:08 INFO: File exists: /home/wagui/stanza_resources/pt/default.zip\n",
      "2022-06-03 14:48:09 INFO: Finished downloading models and saved to /home/wagui/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "import itertools\n",
    "import unidecode\n",
    "\n",
    "stanza.download('pt')\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00556a97-c5f9-4c24-8843-838095de05c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'Science', 'é', 'muito', 'legal']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('Data Science é muito legal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b16ae-70dd-40de-8ecc-55a4f221fa8f",
   "metadata": {},
   "source": [
    "### Trabalhando com Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5914cde2-8b04-4699-8984-88e5791dfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@rodrigofs10: Eu gostei muitoooooooooo desse filme! #movie :-) :-P <3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc88f424-4799-4860-a7b2-a5eebeb66001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'rodrigofs10',\n",
       " ':',\n",
       " 'Eu',\n",
       " 'gostei',\n",
       " 'muitoooooooooo',\n",
       " 'desse',\n",
       " 'filme',\n",
       " '!',\n",
       " '#',\n",
       " 'movie',\n",
       " ':',\n",
       " '-',\n",
       " ')',\n",
       " ':',\n",
       " '-P',\n",
       " '<',\n",
       " '3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebdd826-e796-4b0b-95d4-75ec4f6860a8",
   "metadata": {},
   "source": [
    "### Usando o TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12bc9c5-8555-4085-b19f-45e74d8fdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297bd1eb-42e3-45b3-8bd0-d17bd5d69fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd78320-e14c-4935-be86-5cf2d4475a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@rodrigofs10',\n",
       " ':',\n",
       " 'Eu',\n",
       " 'gostei',\n",
       " 'muitoooooooooo',\n",
       " 'desse',\n",
       " 'filme',\n",
       " '!',\n",
       " '#movie',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokenizer.tokenize(tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95da81-bd45-46a9-9642-61c4ce76937a",
   "metadata": {},
   "source": [
    "### Pré-processamento nos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdfeff5-1daa-43be-bcf8-88cda0d207c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(strip_handles=True, \n",
    "                                 reduce_len=True, \n",
    "                                 preserve_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0988a6a-f342-451f-b04e-120618e24621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " 'eu',\n",
       " 'gostei',\n",
       " 'muitooo',\n",
       " 'desse',\n",
       " 'filme',\n",
       " '!',\n",
       " '#movie',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324dc00e-5f99-485e-beb4-618be2e935c6",
   "metadata": {},
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e120c486-1986-44db-a429-325a85cb115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45142782-cd0c-4e6e-b394-5ee56bca175e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07ddb6a2-9b0f-4ec3-b5ff-4b78fc7bfb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    #tweet_tokenizer = TweetTokenizer()\n",
    "    return text #tweet_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9a4c7-83fa-4753-9919-18d222d9bf83",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9077c66a-6238-4949-8619-341691c69339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    #removing stopwords\n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    stop_words.remove('não')\n",
    "    addicional = [\n",
    "        'ta', 'q', 'nao', 'tah', 'tao', 'eh', 'vc', 'voce',\n",
    "        'pq', 'quedê', 'mane', 'mto', 'mt', 'bj', 'bjs',\n",
    "        'b', 'sao', 'axo', 'mano', 'ae', 'neh', 'aí',\n",
    "        'kkk', 'porque', 'né', 'no']\n",
    "    stop_words.extend(addicional)\n",
    "    return [word for word in text if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edcf734-724c-44d9-a052-3667d168b349",
   "metadata": {},
   "source": [
    "### Stemming and Lammitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0337dd27-1050-4779-a32f-514a38a1c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpers\n",
    "#https://lars76.github.io/2018/05/08/portuguese-lemmatizers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "418a330d-7ce6-4961-adfc-025cbb3bf06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    nlp = stanza.Pipeline('pt')\n",
    "    result = ''\n",
    "    for sent in nlp(' '.join(text)).sentences:\n",
    "        for word in sent.words:\n",
    "            result += word.text + \" \"\n",
    "    \n",
    "   # text = [nlp(word).text for word in text]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f822dfb0-fd00-4a08-a298-8a51638ba008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(text):\n",
    "    #remove punctuation\n",
    "    text = remove_punct(text)\n",
    "    #tokenize the words\n",
    "    text = tokenization(text)\n",
    "    #remove the stop words\n",
    "    text = remove_stopwords(text)\n",
    "    #lemmantize words\n",
    "    text = lemmatizer(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fa0b58ae-7d83-41aa-af27-776a07b6709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean2(tweet):\n",
    "    new_punc = list(string.punctuation)\n",
    "    del new_punc[2]\n",
    "    \n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    stop_words.remove('não')\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\" \\d+\", \"\", tweet)\n",
    "    tweet = re.sub('\\@[a-zA-Z0-9]*', ' ', tweet) # remove username start with @\n",
    "    tweet = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', tweet) # remove link in the tweet\n",
    "    tweet = re.sub('\\$[a-zA-Z0-9]*', ' ', tweet) # remove the variable start with $\n",
    "    #tweet = re.sub('[^a-zA-Z\\']', ' ', tweet)\n",
    "    for punctuation in new_punc: # remove panctuations \n",
    "        tweet = tweet.replace(punctuation, '')\n",
    "    tweet = ' '.join( [w for w in tweet.split() if len(w)>1] ) #remove one letter words\n",
    "    #tweet = ''.join(c[0] for c in itertools.groupby(tweet)) #remove duplicated letters\n",
    "    tweet = unidecode.unidecode(tweet)  # normalizar as letras com acentos \n",
    "    tweet = ' '.join(word for word in tweet.split(' ') if word not in stop_words) # remove stopwords\n",
    "    \n",
    "    #lemantizing\n",
    "    \n",
    "    result = ''\n",
    "    for sent in nlp(tweet).sentences:\n",
    "        for word in sent.words:\n",
    "            result += word.lemma + \" \"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "282785f2-9770-4f9f-be18-9f9a02c7993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = stanza.Pipeline('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ba6f3218-3fd3-4157-be64-8285e4dc39ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'todo vez você chamar grande #hastag '"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean2('Toda vez q você estava me chamando, por isso mesmo no maior @agora #hastag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1cfed-92bb-4adf-90fe-59f9c5283ad3",
   "metadata": {},
   "source": [
    "### Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732ffd1-4b4c-4201-ba21-9862ea126272",
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(df['Tweet'])\n",
    "print('{} Number of tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "#print(countVectorizer.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
