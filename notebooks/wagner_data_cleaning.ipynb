{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd4551a-beb4-44c2-b13c-7fccf8424246",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185cb4e8-7200-4e53-a013-b6a1ec12070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on\n",
    "#https://www.kaggle.com/code/ragnisah/text-data-cleaning-tweets-analysis/notebook\n",
    "#https://github.com/stacktecnologias/stack-repo/blob/master/tweet_tokenize_artigo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7a32cb-a3be-47d3-bc13-b41fd38be909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00556a97-c5f9-4c24-8843-838095de05c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'Science', 'é', 'muito', 'legal']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('Data Science é muito legal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b16ae-70dd-40de-8ecc-55a4f221fa8f",
   "metadata": {},
   "source": [
    "### Trabalhando com Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5914cde2-8b04-4699-8984-88e5791dfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@rodrigofs10: Eu gostei muitoooooooooo desse filme! #movie :-) :-P <3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc88f424-4799-4860-a7b2-a5eebeb66001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'rodrigofs10',\n",
       " ':',\n",
       " 'Eu',\n",
       " 'gostei',\n",
       " 'muitoooooooooo',\n",
       " 'desse',\n",
       " 'filme',\n",
       " '!',\n",
       " '#',\n",
       " 'movie',\n",
       " ':',\n",
       " '-',\n",
       " ')',\n",
       " ':',\n",
       " '-P',\n",
       " '<',\n",
       " '3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebdd826-e796-4b0b-95d4-75ec4f6860a8",
   "metadata": {},
   "source": [
    "### Usando o TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12bc9c5-8555-4085-b19f-45e74d8fdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297bd1eb-42e3-45b3-8bd0-d17bd5d69fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fd78320-e14c-4935-be86-5cf2d4475a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@rodrigofs10',\n",
       " ':',\n",
       " 'Eu',\n",
       " 'gostei',\n",
       " 'muitoooooooooo',\n",
       " 'desse',\n",
       " 'filme',\n",
       " '!',\n",
       " '#movie',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokenizer.tokenize(tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95da81-bd45-46a9-9642-61c4ce76937a",
   "metadata": {},
   "source": [
    "### Pré-processamento nos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdfeff5-1daa-43be-bcf8-88cda0d207c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(strip_handles=True, \n",
    "                                 reduce_len=True, \n",
    "                                 preserve_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0988a6a-f342-451f-b04e-120618e24621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " 'eu',\n",
       " 'gostei',\n",
       " 'muitooo',\n",
       " 'desse',\n",
       " 'filme',\n",
       " '!',\n",
       " '#movie',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324dc00e-5f99-485e-beb4-618be2e935c6",
   "metadata": {},
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e120c486-1986-44db-a429-325a85cb115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45142782-cd0c-4e6e-b394-5ee56bca175e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ddb6a2-9b0f-4ec3-b5ff-4b78fc7bfb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9a4c7-83fa-4753-9919-18d222d9bf83",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9077c66a-6238-4949-8619-341691c69339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    #removing stopwords\n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    stop_words.remove('não')\n",
    "    addicional = [\n",
    "        'ta', 'q', 'nao', 'tah', 'tao', 'eh', 'vc', 'voce',\n",
    "        'pq', 'quedê', 'mane', 'mto', 'mt', 'bj', 'bjs',\n",
    "        'b', 'sao', 'axo', 'mano', 'ae', 'neh', 'aí',\n",
    "        'kkk', 'porque', 'né', 'no']\n",
    "    stop_words.extend(addicional)\n",
    "    return [word for word in text if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f822dfb0-fd00-4a08-a298-8a51638ba008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(text):\n",
    "    #remove punctuation\n",
    "    text = remove_punct(text)\n",
    "    #tokenize the words\n",
    "    text = tokenization(text)\n",
    "    #remove the stop words\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edcf734-724c-44d9-a052-3667d168b349",
   "metadata": {},
   "source": [
    "### Stemming and Lammitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0337dd27-1050-4779-a32f-514a38a1c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpers\n",
    "#https://lars76.github.io/2018/05/08/portuguese-lemmatizers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "418a330d-7ce6-4961-adfc-025cbb3bf06d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatizer\u001b[39m(text):\n\u001b[1;32m      4\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt_core_news_lg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def lemmatizer(text):\n",
    "    nlp = spacy.load(\"pt_core_news_lg\")\n",
    "    \n",
    "    text = [nlp(word) for word in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f3218-3fd3-4157-be64-8285e4dc39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets('Intravenous azithromycin-induced ototoxicity.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
